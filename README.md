# Spark_and_Hadoop_Coding
In this project, I practice write the MapReduce function and execute it on Hadoop system. Next, I also practice SQL on Spark DataFrame by using SparkSQL on Pyspark.

## Environments
- Hadoop 3.1.0
- Spark 2.7.0
- JDK 1.8.0

## Set Up
We need to install the spark and hadoop environment before running the project. Then installing some necessary packages below:
```
pip install findspark
pip install pyspark
```
